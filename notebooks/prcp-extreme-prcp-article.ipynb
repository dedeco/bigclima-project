{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Relatório: Aprendizado de máquina e reconhecimento de padrões</h1>\n",
    "<h3>Aluno: André de Sousa Araújo</h3>\n",
    "<p>Objetivo principal é estimar a ocorrências de precipações extremas.</p>\n",
    "<p>Espaço: Região HOUSTON - Texas</p>\n",
    "<p><b>Base escolhida:</b>\n",
    "This public dataset was created by the National Oceanic and Atmospheric Administration (NOAA) and includes global data obtained from the USAF Climatology Center.  This dataset covers GSOD data between 1929 and present, collected from over 9000 stations.\n",
    "</p>\n",
    "<p>Dataset Source: NOAA</p>\n",
    "\n",
    "<p>Category: Weather</p>\n",
    "\n",
    "<p>Use: This dataset is publicly available for anyone to use under the following terms provided by the Dataset Source — http://www.data.gov/privacy-policy#data_policy — and is provided \"AS IS\" without any warranty, express or implied, from Google. Google disclaims all liability for any damages, direct or indirect, resulting from the use of the dataset.</p>\n",
    "\n",
    "<p>Update Frequency: daily</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Importando o dataset e explorando os dados</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Criando o dataset para treinamento e validação</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Vamos selecionar para treinamento e validação os meses de agosto de 1985 até 2015 desta região.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_query(year):\n",
    "  base_query = \"\"\"\n",
    "       SELECT    d.stn\n",
    "                 ,ws.lat\n",
    "                 ,ws.lon\n",
    "                 ,d.da \n",
    "                 ,d.mo\n",
    "                 ,d.year \n",
    "                 ,d.temp\n",
    "                 ,d.dewp\n",
    "                 ,d.slp\n",
    "                 ,d.stp\n",
    "                 ,d.visib\n",
    "                 ,d.wdsp\n",
    "                 ,d.mxpsd\n",
    "                 ,d.gust\n",
    "                 ,d.max\n",
    "                 ,d.min\n",
    "                 ,d.prcp\n",
    "                 ,d.flag_prcp\n",
    "                 ,d.sndp\n",
    "                 ,d.fog\n",
    "                 ,d.rain_drizzle\n",
    "                 ,d.snow_ice_pellets\n",
    "                 ,d.hail\n",
    "                 ,d.thunder\n",
    "                 ,d.tornado_funnel_cloud\n",
    "          FROM \n",
    "            [bigquery-public-data:noaa_gsod.gsod{0}] d\n",
    "            JOIN [bigquery-public-data:noaa_gsod.stations] ws\n",
    "              ON d.stn = ws.usaf\n",
    "              and d.wban = ws.wban\n",
    "          where ws.state is not null\n",
    "              and ws.country = 'US'\n",
    "              and INTEGER(d.mo) in (6,7,8)\n",
    "          order by d.stn, d.da\n",
    "    \"\"\"\n",
    "  query = base_query.format(year)\n",
    "  return query\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "for y in range(2015,2016):\n",
    "  r = bq.Query(create_query(y)).to_dataframe()\n",
    "  df = df.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Salvando os dados</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_csv(df, filename, target):\n",
    "  outdf = df.copy(deep=False)\n",
    "  outdf.loc[:, 'key'] = np.arange(0, len(outdf)) # rownumber as key\n",
    "  # reorder columns so that target is first column\n",
    "  cols = outdf.columns.tolist()\n",
    "  cols.remove(target)\n",
    "  cols.insert(0, target)\n",
    "  #print (cols)  # new order of columns\n",
    "  outdf = outdf[cols]\n",
    "  outdf.to_csv(filename, header=True, index_label=False, index=False)\n",
    "\n",
    "to_csv(df, '../data/prcp-data-2015.csv','d_prcp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">Verificando a distribuição do conjunto de treinamento e validação</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [14, 8]  \n",
    "prcp30_limpo = df[df['d_prcp']< 99.99]\n",
    "prcp30_limpo.d_prcp.hist()  \n",
    "plt.title(u'Distribuição da precipitação - AGO 1985 a 2015 - região Houston')  \n",
    "plt.xlabel(u'Precipitação (polegadas)')  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">Limiar para classificar a precipitação como extrema do conjunto de treinamento e validação</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "p = np.percentile(prcp30_limpo['d_prcp'], 99) # return 99%th percentile\n",
    "print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Considerando os últimos 30 anos o limiar é 1.65 polegadas. Comparando com 2016 é limiar mais baixo, 2016 teve um agosto bem mais chuvoso e com mais eventos extremos que a média dos últimos anos</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">Medindo a quantidade de chuvas extremas no período</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extreme = prcp30_limpo[prcp30_limpo['d_prcp']> 1.3] \n",
    "r = extreme.groupby(['d_year'])['d_prcp'].count()\n",
    "r.plot.bar()\n",
    "plt.title(u'Preciptações extremas - AGO 1985 a 2015 - região Houston - limiar (99%)')  \n",
    "plt.ylabel(u'Qtde')  \n",
    "plt.xlabel(u'Ano')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Feature engineering</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>O objetivo é realizar a previsão destes eventos usando as variáveis de 3 dias atrás. No estudo de Dolif Neto e Nobre (2012) foram extraídas variavéis também de 3 dias atrás dos fenômenos precipitações extremas. Assim vamos derivar as variáveis de 3 dias atrás antes do fenômeno. Inicialente <b>vamos derivar somente a precipitação</b>, testar e depois fazer para todas as varíaveis</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prcp30_limpo = prcp30_limpo.apply(pd.to_numeric, errors='coerce')  \n",
    "\n",
    "tmp = prcp30_limpo[['d_stn','d_da','d_mo','d_year','d_prcp']]\n",
    "tmp = tmp.sort_values(['d_year','d_stn','d_da'])\n",
    "tmp = tmp.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N = 1 #1 dia antes\n",
    "feature = 'd_prcp'\n",
    "\n",
    "nth_prior =[]\n",
    "\n",
    "# Devirando a variavel para uma lista representando a enesima valor da variavel, se for o primeiro dia não terá dia anterior.\n",
    "for y in range(2015,2016):\n",
    "  df_year = tmp[tmp['d_year']== y]\n",
    "  df_year = df_year.reset_index(drop=True)\n",
    "  rows = df_year.shape[0]\n",
    "  prior= [None]*N + [df_year[feature][i-N] for i in range(N, rows)]\n",
    "  nth_prior = nth_prior + prior\n",
    "  \n",
    "col_name = \"{}_{}\".format(feature, N)  \n",
    "tmp[col_name] = nth_prior  \n",
    "tmp[:10]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Veja que para a estação ID 722430 no dia 4/8/1985, a precipitação do dia anterior (d_prcp_1) é exatamente o valor medido no dia 03/08/1985. Agora vamos fazer isto para todas as variaveis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def derivar_valor_anterior(df, dias, variavel):\n",
    "  N = dias\n",
    "  feature = variavel\n",
    "  tmp = df\n",
    "\n",
    "  nth_prior =[]\n",
    "\n",
    "  # Devirando a variavel para uma lista representando a enesima valor da variavel, se for o primeiro dia não terá dia anterior.\n",
    "  for y in range(2015,2016):\n",
    "    df_year = tmp[tmp['d_year']== y]\n",
    "    df_year = df_year.reset_index(drop=True)\n",
    "    rows = df_year.shape[0]\n",
    "    prior= [None]*N + [df_year[feature][i-N] for i in range(N, rows)]\n",
    "    nth_prior = nth_prior + prior\n",
    "\n",
    "  col_name = \"{}_{}\".format(feature, N)  \n",
    "  tmp[col_name] = nth_prior\n",
    "\n",
    "\n",
    "df_dev = df.apply(pd.to_numeric, errors='coerce')  \n",
    "df_dev = df_dev.sort_values(['d_year','d_stn','d_da'])\n",
    "df_dev = df_dev.reset_index(drop=True)\n",
    "  \n",
    "for var in list(df_dev):\n",
    "  if var not in ['d_stn','d_da','d_mo','d_year']:\n",
    "    for N in range(1, 4):\n",
    "      derivar_valor_anterior(df_dev,N,var)\n",
    "\n",
    "df_dev.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Pronto! Agora temos variaveis respectivas de 1, 2, 3 dias atrás. Exemplo: d_prcp_1, d_prcp_2, d_prcp_3 referentes a precipitação 1, 2 e 3 dias atrás respectivamente.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Limpeza dos dados</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Conforme dito anteriormente, existem muitos dados ausentes para algumas variáveis. Podemos excluir as instâncias, mas precisamos ver se perderíamos muitos dados</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dev = df_dev.apply(pd.to_numeric, errors='coerce')  \n",
    "df_dev.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_dev[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_temp = df_dev[(df_dev < 9999.9000).all(axis=1)]\n",
    "print ('Tamanho antes da limpeza: %s' %str(len(df_dev))) \n",
    "print ('Tamanho depois da limpeza: %s' %str(len(df_temp))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A conclusão é que todo registro tem algum dado ausente. Assim vamos verificar se tem alguma variável que apresenta maiores problemas.<p/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def contabilizar_dados_ausentes(df):\n",
    "  dn = {}\n",
    "  \n",
    "  for v in df.columns:\n",
    "    df_grp = df[df[v] == 9999.9]\n",
    "    dn[v] = len(df_grp)\n",
    "\n",
    "  for v in ['d_gust','d_gust_1', 'd_gust_2', 'd_gust_3']:\n",
    "    if v in df.columns:\n",
    "      df_grp = df[df[v] == 999.9]\n",
    "      dn[v] = dn[v] + len(df_grp)\n",
    "\n",
    "  for v in ['d_prcp','d_prcp_1', 'd_prcp_2', 'd_prcp_3']:\n",
    "    if v in df.columns:\n",
    "      df_grp = df[df[v] == 99.9]\n",
    "      dn[v] = dn[v] + len(df_grp) \n",
    "    \n",
    "  pdn = pd.DataFrame(dn.items(), columns=['freatures', 'missing'])\n",
    "  pdn = pdn[pdn > 0]\n",
    "  pdn = pdn.sort_values(['missing'])\n",
    "  \n",
    "  return pdn\n",
    "\n",
    "pdn = contabilizar_dados_ausentes(df_dev)\n",
    "pdn.plot.bar(x='freatures', y='missing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> As varíveis velocidade máxima do vento do dia (d_mxpsd), velocidade do fenômeno de aumento repentino do vento (d_gust), média da velocidade do vento (d_wdsp), temperatura do ponto de orvaloho (d_dewp) e pressão atmosferica (d_stp) apresentam valores nulos significativos. Sendo a variável d_gust a mais problemática.</p>\n",
    "\n",
    "<p>A decisão será remover velocidade do fenômeno de aumento repentino do vento (d_gust) e  pressão atmosferica (d_stp). E interpolar as demais utilizando algum método.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_limpo = df_dev.copy(deep=True)\n",
    "\n",
    "for column in df_limpo.columns:\n",
    "    if column in ['d_gust','d_gust_1','d_gust_2','d_gust_3']:\n",
    "        df_limpo = df_limpo.drop(column, axis = 1)\n",
    "        \n",
    "for column in df_limpo.columns:\n",
    "    if column in ['d_stp','d_stp_1','d_stp_2','d_stp_3']:\n",
    "        df_limpo = df_limpo.drop(column, axis = 1)\n",
    "        \n",
    "for column in df_limpo.columns:\n",
    "    if column in ['d_flag_prcp','d_flag_prcp_1','d_flag_prcp_2','d_flag_prcp_3']:\n",
    "        df_limpo = df_limpo.drop(column, axis = 1)\n",
    "        \n",
    "for column in df_limpo.columns:\n",
    "    if column in ['d_sndp','d_sndp_1','d_sndp_2','d_sndp_3']:\n",
    "        df_limpo = df_limpo.drop(column, axis = 1)\n",
    "\n",
    "df_limpo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Agora temos outras varíaveis para tratar, que ainda estão afetando muitas instâncias</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = contabilizar_dados_ausentes(df_limpo)\n",
    "p.plot.bar(x='freatures', y='missing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Primeiramente vamos marcar os dados ausentes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def marcar_dados_ausentes(df):\n",
    "  for v in df.columns:\n",
    "    df[v] = df[v].replace([9999.9],\"NaN\")\n",
    "  for v in df.columns:\n",
    "    df[v] = df[v].replace([999.9],\"NaN\")\n",
    "\n",
    "marcar_dados_ausentes(df_limpo)\n",
    "\n",
    "for v in df_limpo.columns:\n",
    "  if v in ['d_prcp','d_prcp_1', 'd_prcp_2', 'd_prcp_3']:\n",
    "    df_limpo[v] = df_limpo[v].replace([99.9],\"NaN\")\n",
    "\n",
    "df_limpo = df_limpo.apply(pd.to_numeric, errors='coerce')  \n",
    "df_limpo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Interpolá-los</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Interpolandos dados para outras variáveis: dados ausentes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_intptd = df_limpo.copy(deep=True)\n",
    "for v in df_intptd:\n",
    "  if v not in ['d_stn','d_da','d_mo','d_year']:\n",
    "    df_intptd[v] = df_intptd[v].astype(float)\n",
    "\n",
    "df_intptd = df_intptd.interpolate(method='linear', axis=0).ffill().bfill()\n",
    "df_intptd.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_intptd[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Adicionando um flag para indicar se a chuva é extrema ou não</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Adicionando o flag para indicar se a chuva é extrema ou não</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_intptd['heavy'] = (df_intptd['d_prcp'] > 1.6548)\n",
    "df_intptd['heavy']  = df_intptd['heavy'].astype(object).replace({False: 0, True: 1})\n",
    "df_intptd[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Pearson correlation coefficient</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_corl = df_intptd.copy(deep=True)\n",
    "for v in df_corl.columns:\n",
    "  if v in ['d_stn','d_da','d_mo','d_year']:\n",
    "    df_corl = df_corl.drop(v, axis = 1)\n",
    "\n",
    "df_corl.corr()[['d_prcp']].sort_values('d_prcp')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Nenhuma variável apresentou uma correlação alta. Assim vamos aplicar um modelo não linear.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Treinamento e validação</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_intptd[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Colocando a variável alvo na primeira posição do dataframe</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TARGET = u'd_prcp'\n",
    "#TARGET = u'heavy'\n",
    "\n",
    "cols = list(df_intptd)\n",
    "cols.remove(TARGET)\n",
    "cols.insert(0, TARGET)\n",
    "print (cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for v in df_intptd.columns:\n",
    "  if v in ['d_da','d_mo','d_year']:\n",
    "    df_intptd = df_intptd.drop(v, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_intptd[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "FEATURES = df_intptd.columns.tolist()\n",
    "\n",
    "FLAGS = ['d_fog','d_rain_drizzle','d_snow_ice_pellets','d_hail','d_thunder','d_tornado_funnel_cloud']\n",
    "\n",
    "for f in FLAGS:\n",
    "  FLAGS.append(f+'_1')\n",
    "  FLAGS.append(f+'_2')\n",
    "  FLAGS.append(f+'_3')\n",
    "\n",
    "print FLAGS \n",
    "\n",
    "SPARSE = ['d_stn']\n",
    "\n",
    "for f in FLAGS:\n",
    "  FEATURES.remove(f)\n",
    "  \n",
    "print FEATURES\n",
    "\n",
    "FEATURES.remove(SPARSE)\n",
    "\n",
    "FEATURES.remove(TARGET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_SIZE = int(0.7*len(df_intptd))\n",
    "df_train = df_intptd[:TRAIN_SIZE]\n",
    "df_valid = df_intptd[TRAIN_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red;\">Funções para ler o dataframe para inserir no tf.constant</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_input_fn(df):\n",
    "  def pandas_to_tf(pdcol):\n",
    "    # convert the pandas column values to float\n",
    "    t = tf.constant(pdcol.astype('float32').values)\n",
    "    # take the column which is of shape (N) and make it (N, 1)\n",
    "    return tf.expand_dims(t, -1)\n",
    "  \n",
    "  def input_fn():\n",
    "    # create features, columns\n",
    "    features = {k: pandas_to_tf(df[k]) for k in FEATURES}\n",
    "    labels = tf.constant(df[TARGET].values)\n",
    "    return features, labels\n",
    "  return input_fn\n",
    "  \n",
    "def make_feature_cols():\n",
    "  print FEATURES\n",
    "  input_columns = [tf.contrib.layers.real_valued_column(k) for k in FEATURES]\n",
    "  return input_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\">Regressão Linear</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "shutil.rmtree('prcp', ignore_errors=True) # start fresh each time\n",
    "model = tf.contrib.learn.LinearRegressor(feature_columns=make_feature_cols(), model_dir='prcp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(input_fn=make_input_fn(df_train), steps=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_rmse(model, name, input_fn):\n",
    "  metrics = model.evaluate(input_fn=input_fn, steps=1)\n",
    "  print u'RMSE(Root Mean Squared Error) no {} = {}'.format(name, np.sqrt(metrics['loss']))\n",
    "  \n",
    "print_rmse(model, u'conjunto de validação', make_input_fn(df_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = tf.contrib.learn.LinearRegressor(feature_columns=make_feature_cols(), model_dir='prcp')\n",
    "preds = model.predict(input_fn=make_input_fn(df_valid))\n",
    "m = list(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Comparando as predições com o conjunto de validação.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_comparar = df_valid.copy(deep=True)\n",
    "df_comparar = df_comparar.reset_index(drop=True)\n",
    "\n",
    "df_comparar['resultado'] = m\n",
    "\n",
    "cols = list(df_comparar)\n",
    "cols.insert(0, cols.pop(cols.index('resultado')))\n",
    "df_comparar = df_comparar.ix[:, cols]\n",
    "\n",
    "print df_comparar[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p><b>Root Mean Square Error (RMSE)</b> is the standard deviation of the residuals (prediction errors). Residuals are a measure of how far from the regression line data points are; RMSE is a measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit. Root mean square error is commonly used in climatology, forecasting, and regression analysis to verify experimental results.</P>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:green;\"> Deep Neural Network regression </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shutil.rmtree('prcp', ignore_errors=True)\n",
    "model = tf.contrib.learn.DNNRegressor(hidden_units=[32, 8, 2], feature_columns=make_feature_cols(), \n",
    "                                      optimizer=tf.train.ProximalAdagradOptimizer(learning_rate=0.3,l1_regularization_strength=0.001),\n",
    "                                      model_dir='prcp', )\n",
    "model.fit(input_fn=make_input_fn(df_train), steps=500);\n",
    "print_rmse(model, u'conjunto de validação', make_input_fn(df_valid))\n",
    "preds = model.predict(input_fn=make_input_fn(df_valid))\n",
    "m = list(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_comparar = df_valid.copy(deep=True)\n",
    "df_comparar = df_comparar.reset_index(drop=True)\n",
    "\n",
    "df_comparar['resultado'] = m\n",
    "\n",
    "cols = list(df_comparar)\n",
    "cols.insert(0, cols.pop(cols.index('resultado')))\n",
    "df_comparar = df_comparar.ix[:, cols]\n",
    "\n",
    "print df_comparar[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\">Conclusão</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Os resultados não foram bons, visto que são viciados, mas a proposta é rodar novamente mas desta vez aplicando validação cruzada. E refazer o holdout mas feito corretamente, ou seja, selecionar randomicamente. Aplicando o modelo regressão linear o <b>RMSE(Root Mean Squared Error) foi 22.68 </b> e aplicando redes neurais <b>(DNN Regressor) foi 24.66 </b>.</p>\n",
    "\n",
    "<p>Os próximos passos são fazer o balanciamento, e possivelmente discretizar a chuva extrema em intervalos ao inves de prever a precipitação em valores contínuos. E selecionar as instâncias de forma randômica.</p>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
